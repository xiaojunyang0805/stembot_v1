# WP3-4.1: Comprehensive Literature Review System Testing

## Test Execution Date
**Planned:** October 1, 2025
**Status:** Ready for Execution

---

## Testing Scenarios

### Scenario 1: Chemistry Research - Buffer Solution Effectiveness

**Objective:** Test complete workflow with chemistry research project

**Steps:**
1. **Project Creation**
   - Navigate to: https://stembotv1.vercel.app/projects/create
   - Title: "Buffer solution effectiveness in maintaining pH stability"
   - Research Question: "How do different buffer compositions affect pH stability under temperature variations?"
   - Expected: Project created successfully

2. **Search Strategy Generation**
   - Navigate to Literature Review page
   - Click "Generate Search Strategy"
   - **Expected Results:**
     - Suggests chemistry-specific databases (PubChem, ACS Publications, RSC)
     - Generates keywords: "buffer capacity", "pH stability", "temperature effects", "Henderson-Hasselbalch"
     - Completion time: <5 seconds
   - **Success Criteria:** âœ“ Chemistry-relevant terms generated âœ“ Database suggestions appropriate

3. **Upload Chemistry Papers**
   - Upload 5 PDF chemistry papers (various quality levels)
   - **Expected Results:**
     - PDF processing completes: <30 seconds per paper
     - Metadata extraction: authors, year, journal, DOI
     - Text extraction for credibility assessment
   - **Success Criteria:** âœ“ All 5 papers uploaded âœ“ Metadata extracted âœ“ Ready for assessment

4. **Credibility Assessment**
   - Trigger credibility assessment for all sources
   - **Expected Results:**
     - Assessment completes: <10 seconds per source
     - Scores range from 60-95 (realistic for chemistry papers)
     - Identifies: peer-reviewed status, journal impact, methodology quality
     - Labels: High/Moderate/Low credibility
   - **Verification Points:**
     - High-quality journal (Nature Chemistry, JACS) â†’ High credibility (85-95)
     - Mid-tier journal â†’ Moderate (70-84)
     - Preprint/non-peer-reviewed â†’ Low (<70)
   - **Success Criteria:** âœ“ Scores make sense âœ“ Peer-review detected âœ“ Methodology evaluated

5. **Gap Analysis**
   - Run gap analysis on 5 sources
   - **Expected Results:**
     - Analysis completes: <15 seconds
     - Identifies 3-5 research gaps
     - Gap types: methodology, temperature range, buffer types, pH ranges
     - Provides actionable suggestions
   - **Sample Expected Gaps:**
     - "Limited studies on extreme temperature ranges (>50Â°C)"
     - "Few comparisons of phosphate vs. acetate buffers"
     - "Lack of long-term stability data"
   - **Success Criteria:** âœ“ Real gaps identified âœ“ Chemistry-specific âœ“ Actionable recommendations

6. **Workspace Integration**
   - Navigate to project workspace
   - **Expected Results:**
     - ProjectMemoryPanel shows: "ðŸ“š Sources: 5"
     - Shows latest source title and authors
     - Gap count displayed
     - Progress percentage: ~60% (5 sources, gap analysis done)
   - **Success Criteria:** âœ“ All metrics visible âœ“ Latest source shown âœ“ Link to literature page works

7. **Chat Integration Test**
   - Ask: "What do my sources say about buffer capacity?"
   - **Expected Results:**
     - AI retrieves relevant sources
     - Response includes citations: "Based on Smith et al. (2024)..."
     - References specific findings from uploaded papers
     - Completion time: <5 seconds
   - **Success Criteria:** âœ“ Citations included âœ“ Relevant findings referenced âœ“ Natural language

---

### Scenario 2: Biology Research - Bacterial Growth Under Stress

**Objective:** Test with mixed-quality sources and progressive gap analysis

**Steps:**
1. **Project Creation**
   - Title: "Bacterial growth and survival under oxidative stress"
   - Research Question: "How do different bacterial species adapt to oxidative stress conditions?"

2. **Upload Mixed Quality Sources**
   - **Source Mix:**
     - 2 high-quality: Cell, Nature Microbiology (peer-reviewed, high impact)
     - 2 moderate: PLOS ONE, BMC Microbiology (peer-reviewed, moderate impact)
     - 1 low-quality: bioRxiv preprint (not peer-reviewed)

3. **Credibility System Verification**
   - **Expected Categorization:**
     - High-quality journals â†’ 85-95 credibility, "High" label
     - PLOS ONE/BMC â†’ 70-84 credibility, "Moderate" label
     - bioRxiv preprint â†’ 50-69 credibility, "Low" label
   - **Verification:**
     - Check peer-review status detection
     - Verify impact factor influence
     - Confirm methodology scoring
   - **Success Criteria:** âœ“ Correct categorization âœ“ Appropriate scores âœ“ Clear explanations

4. **Progressive Gap Analysis**
   - **Test 1: 3 sources**
     - Run gap analysis
     - Expected: 2-3 broad gaps identified
     - Time: <10 seconds

   - **Test 2: 5 sources**
     - Add 2 more sources
     - Run gap analysis again
     - Expected: 3-4 more specific gaps
     - Time: <15 seconds

   - **Test 3: 10 sources**
     - Add 5 more sources
     - Run gap analysis
     - Expected: 4-6 refined gaps with specific recommendations
     - Time: <20 seconds

   - **Verification:**
     - Gaps become more specific with more sources
     - Recommendations become more actionable
     - No duplicate gap identification

   - **Success Criteria:** âœ“ Progressive refinement âœ“ Timing requirements met âœ“ Quality improves

5. **Methodology Phase Access**
   - Navigate to Methodology phase
   - Click "View Literature Recommendations"
   - **Expected Results:**
     - 2-3 methodology recommendations appear
     - Based on patterns from uploaded sources
     - Includes: experimental design, statistical approaches
     - Links back to specific sources
   - **Success Criteria:** âœ“ Recommendations visible âœ“ Based on literature âœ“ Actionable steps

---

### Scenario 3: Cross-Session Continuity

**Objective:** Verify persistence and real-time updates across sessions

**Test Steps:**

**Day 1 - Initial Session:**
1. Create project: "Social media impact on academic performance"
2. Upload 3 sources (psychology papers)
3. Run credibility assessment
4. Check ProjectMemoryPanel: shows 3 sources
5. Close browser completely
6. **Expected:** All data saved to Supabase

**Day 2 - Return Session:**
1. Open browser, login
2. Navigate to project
3. **Verification Points:**
   - âœ“ 3 sources still visible in literature page
   - âœ“ Credibility scores preserved
   - âœ“ ProjectMemoryPanel shows "Sources: 3"
   - âœ“ All metadata intact (authors, titles, DOIs)
4. Add 2 more sources
5. **Expected:**
   - ProjectMemoryPanel updates to "Sources: 5"
   - Progress percentage increases
   - Latest source updates automatically

**Gap Analysis Auto-Update:**
1. Before adding sources: Note gap analysis results (if any)
2. Add 2 new sources
3. Trigger gap analysis
4. **Expected:**
   - New analysis incorporates all 5 sources
   - Gaps refined based on new data
   - Dashboard progress updates automatically
   - Methodology recommendations update

**Dashboard Progress Check:**
1. Navigate to dashboard
2. **Verification:**
   - Project card shows literature progress
   - Source count: 5
   - Progress bar reflects literature completion
   - "View Project" link works
   - Literature milestone: "âœ“ Initial sources collected"

**Success Criteria:**
- âœ“ Data persists across sessions
- âœ“ No data loss after browser close
- âœ“ Real-time updates work
- âœ“ Dashboard reflects current state
- âœ“ Gap analysis incorporates all sources

---

## Performance Testing

### Timing Requirements

| Operation | Target Time | Test Method | Pass/Fail |
|-----------|-------------|-------------|-----------|
| **Search term generation** | <5 seconds | Use Chrome DevTools Network tab | |
| **PDF processing** | <30 seconds/file | Monitor upload progress | |
| **Credibility assessment** | <10 seconds/source | Time from trigger to display | |
| **Gap analysis (5-10 sources)** | <15 seconds | Time from click to results | |
| **Page load with 10 sources** | <2 seconds | Lighthouse Performance score >90 | |
| **Memory retrieval** | <500ms | Check API response time | |
| **Chat context retrieval** | <3 seconds | Time from query to response start | |

### Performance Test Execution

**Test 1: Search Term Generation**
```javascript
// In browser console
console.time('searchGeneration');
// Click "Generate Search Strategy"
// When results appear:
console.timeEnd('searchGeneration');
// Expected: <5000ms
```

**Test 2: Credibility Assessment**
```javascript
console.time('credibilityAssessment');
// Trigger assessment for one source
console.timeEnd('credibilityAssessment');
// Expected: <10000ms
```

**Test 3: Gap Analysis**
```javascript
console.time('gapAnalysis');
// Click "Analyze Gaps" with 5 sources
console.timeEnd('gapAnalysis');
// Expected: <15000ms
```

**Test 4: Page Load Performance**
- Open Chrome DevTools
- Go to Lighthouse tab
- Run performance audit on `/projects/[id]/literature` with 10 sources
- **Expected Scores:**
  - Performance: >90
  - First Contentful Paint: <1.5s
  - Time to Interactive: <3.5s

**Test 5: API Response Times**
- Open Network tab in DevTools
- Filter: XHR/Fetch
- Check response times:
  - `GET /api/projects/[id]/sources` â†’ <500ms
  - `POST /api/ai/credibility-assessment` â†’ <10s
  - `POST /api/research/gap-analysis` â†’ <15s

---

## Functionality Checklist

### âœ… Core Features

**1. Search Terms Generation**
- [ ] Generates based on research question
- [ ] Suggests appropriate databases for field
- [ ] Includes synonyms and related terms
- [ ] Provides 8-12 keyword suggestions
- [ ] Completion time: <5 seconds

**2. Source Upload & Processing**
- [ ] PDF upload works correctly
- [ ] Metadata extraction (title, authors, year, DOI)
- [ ] Text extraction for analysis
- [ ] Multiple file upload support
- [ ] Progress indicators visible

**3. Credibility Assessment**
- [ ] Displays credibility score (0-100)
- [ ] Shows High/Moderate/Low label
- [ ] Lists credibility factors (peer-reviewed, impact, citations)
- [ ] Provides explanation of score
- [ ] Detects study type/methodology
- [ ] Timing: <10 seconds per source

**4. Gap Analysis**
- [ ] Identifies 3-6 research gaps
- [ ] Categorizes gap types (methodology, population, timeframe, etc.)
- [ ] Provides actionable suggestions
- [ ] Links gaps to potential research questions
- [ ] Updates when sources added
- [ ] Timing: <15 seconds for 5-10 sources

**5. Project Memory Integration**
- [ ] Sources save to project database
- [ ] Metadata persists across sessions
- [ ] Source count updates in real-time
- [ ] ProjectMemoryPanel displays correctly
- [ ] Latest source preview shows
- [ ] Gap count displayed
- [ ] Progress percentage calculated

**6. Workspace Display**
- [ ] Literature progress shows in sidebar
- [ ] Source count visible
- [ ] Gap analysis summary displayed
- [ ] Next action recommendations shown
- [ ] Link to literature page works
- [ ] Visual progress indicators

**7. Chat Integration**
- [ ] AI retrieves relevant sources by query
- [ ] Responses include source citations
- [ ] Format: "Based on Smith (2024)..."
- [ ] References specific findings
- [ ] Answers "What do my sources say about X?"
- [ ] Response time: <5 seconds

**8. Dashboard Integration**
- [ ] Literature progress shows on project card
- [ ] Source count displayed
- [ ] Milestones tracked
- [ ] Progress bar updates
- [ ] Click-through to literature page works

**9. Cross-Phase Access**
- [ ] Methodology phase can access sources
- [ ] Recommendations based on literature
- [ ] Writing phase can retrieve citations
- [ ] Citation database available
- [ ] Memory context shared across phases

**10. Data Persistence**
- [ ] Sources survive browser close
- [ ] Credibility scores preserved
- [ ] Gap analysis results saved
- [ ] Progress data persists
- [ ] No data loss across sessions

---

## Edge Cases & Error Handling

### Test Edge Cases

**1. No Sources Yet**
- Navigate to literature page with 0 sources
- Expected: Empty state with "Add sources" CTA
- No errors in console

**2. Large PDF (>10MB)**
- Upload large research paper
- Expected: Progress indicator, warning if too large
- Graceful handling or clear error message

**3. Corrupted PDF**
- Upload corrupted/password-protected PDF
- Expected: Clear error message
- Doesn't break page

**4. No Internet During Upload**
- Disconnect internet mid-upload
- Expected: Retry mechanism or clear failure message
- Resume capability if possible

**5. Duplicate Sources**
- Upload same paper twice
- Expected: Detection and warning
- Option to skip or overwrite

**6. Gap Analysis with 0-2 Sources**
- Trigger gap analysis with insufficient sources
- Expected: Message: "Need 3+ sources for meaningful gap analysis"
- No errors

**7. Chat Without Sources**
- Ask literature question with 0 sources
- Expected: AI responds: "No sources added yet. Add sources to get literature-based insights."
- Graceful degradation

---

## Test Execution Report Template

### Test Session Information
- **Tester:** [Name]
- **Date:** [Date]
- **Time:** [Start - End]
- **Environment:** Production (https://stembotv1.vercel.app)
- **Browser:** Chrome [version]
- **Device:** [Desktop/Mobile]

### Results Summary

**Scenario 1: Chemistry Research**
- Status: [ ] Pass [ ] Fail [ ] Partial
- Notes:
- Issues Found:

**Scenario 2: Biology Research**
- Status: [ ] Pass [ ] Fail [ ] Partial
- Notes:
- Issues Found:

**Scenario 3: Cross-Session Continuity**
- Status: [ ] Pass [ ] Fail [ ] Partial
- Notes:
- Issues Found:

**Performance Testing**
- Search Generation: [time] - [ ] Pass [ ] Fail
- PDF Processing: [time] - [ ] Pass [ ] Fail
- Credibility Assessment: [time] - [ ] Pass [ ] Fail
- Gap Analysis: [time] - [ ] Pass [ ] Fail
- Page Load: [time] - [ ] Pass [ ] Fail
- Memory Retrieval: [time] - [ ] Pass [ ] Fail

**Functionality Checklist**
- Items Passed: [X/10]
- Items Failed: [List]
- Partially Working: [List]

### Issues Log

| Issue # | Severity | Description | Steps to Reproduce | Expected | Actual | Status |
|---------|----------|-------------|-------------------|----------|--------|--------|
| 1 | | | | | | |
| 2 | | | | | | |

### Recommendations
1.
2.
3.

---

## Automated Test Suite (Optional)

For future CI/CD integration, here's a test suite structure:

```typescript
// tests/e2e/literature-review.spec.ts

describe('WP3-4.1: Literature Review System', () => {
  describe('Scenario 1: Chemistry Research', () => {
    it('should create project successfully', async () => {
      // Test implementation
    });

    it('should generate chemistry-specific search terms', async () => {
      // Test implementation
    });

    it('should upload and process PDFs', async () => {
      // Test implementation
    });

    it('should assess credibility correctly', async () => {
      // Test implementation
    });

    it('should identify meaningful gaps', async () => {
      // Test implementation
    });
  });

  describe('Performance Tests', () => {
    it('should generate search terms in <5 seconds', async () => {
      // Test implementation
    });

    it('should complete credibility assessment in <10 seconds', async () => {
      // Test implementation
    });
  });
});
```

---

## Sign-Off

**Testing Completed By:** ___________________
**Date:** ___________________
**Overall Status:** [ ] Pass [ ] Fail [ ] Needs Rework
**Approved for Production:** [ ] Yes [ ] No

---

## Next Steps After Testing

Based on test results:
1. **If All Pass:** Proceed to WP3-4.2 (User Feedback Integration)
2. **If Issues Found:**
   - Document all issues
   - Prioritize by severity
   - Fix critical issues
   - Re-test
3. **Performance Issues:** Optimize slow operations
4. **Edge Cases:** Add error handling for identified cases
